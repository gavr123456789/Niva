type Parser
  tokens: List::Token
  current: Int
  statements: mut List::Statement

constructor Parser newParseTokens::List::Token = [
  statements::mut List::Statement = {}!
  parser2:: mut Parser = Parser tokens: newParseTokens current: 0 statements: statements // {}! error TODO

  [ parser2 done not ] whileTrue: [
    parser2 statements add: parser2 nextStatement
  ]

  // parser statements debug

  ^ parser2
]


type TypeField name: String astType: AstType2
TypeField toString -> String =  "$name: $astType"

extend mut Parser [
  // maybe collections are missing
  on isLiteralExpr(tok): Token -> Bool = [
   ^| tok kind
    | TokenType.Integer
    | TokenType.String
    | TokenType.True|TokenType.False
    | TokenType.Char
    | TokenType.Float
    | TokenType.Double
    // | TokenType.Unit
    | TokenType.Null => true
    |=> false
  ]

  on parseReturnExpr(tok): Token -> ReturnStatement = [
    tok kind == TokenType.Return ifFalse: [TO DO: "^ expected"]

    // is that `^ value` or just `^`
    savePosition = current
    isThereAValue = (.primary: .peek) != null
    .restoreFrom: savePosition

    value = isThereAValue ifTrue: [.nextExperssion: .step] ifFalse: [null]

    result = ReturnStatement value: value token: tok
    ^ result
  ]

  /// tok is identifier
  /// pasre: x = expr
  on parseVarDeclExpr(token): Token name::String isMut::Bool -> VarDeclaration = [
    astType = token kind == TokenType.IdentifierDoubleColon ifTrue: [
      .parseType
    ] ifFalse: [null]

    .matchAssert: TokenType.Assign
    value = .nextExperssion: .step
    result = VarDeclaration :name :value :token :astType :isMut
    ^ result
  ]

  /// a^.b.c
  on parseDotSeparatedIdentifier(tok): Token -> Identifier = [
    ^ .match: TokenType.Dot, ifTrue: [

      path = {(tok lexeme)}!
      mut c = true
      [c] whileTrue: [
        ident = .matchAssert: TokenType.Identifier
        path add: ident lexeme
        c <- .match: TokenType.Dot
      ]
      Identifier name: tok lexeme token: tok path: path
    ] ifFalse: [
      Identifier name: tok lexeme token: tok path: null
    ]
  ]

  /// "", 1, 1.1, true, []
  on literalExprFrom(tok): Token -> LiteralExpr? = [
    mut isAdded = false

    x = | tok kind
    | TokenType.Integer =>
      IntExpr num: tok lexeme toInt token: tok

    | TokenType.String =>
      StringExpr token: tok

    | TokenType.True|TokenType.False =>
      BooleanExpr token: tok

    | TokenType.Char =>
      CharExpr token: tok

    | TokenType.Float =>
      FloatExpr token: tok

    | TokenType.Double =>
      DoubleExpr token: tok

    | TokenType.Null =>
      NullExpr token: tok

    // | TokenType.Unit =>
    //   UnitExpr token: tok

    |=> [null]
    ^ x
  ]

  /// if starts from { #{ #(, then return collection
  /// tok already eaten outside
  on parseCollection(tok): Token -> CollectionLiteral? = [

    manyPrimary = [
      body::mut List::Expr = {}!

      mut c = true
      [c] whileTrue: [
        tok2 = .step
        pr = .primary: tok2
        | pr
        | null => [
          c <- false
        ]
        |=> [
          body add: pr
          .match: TokenType.Comma
          Unit
        ]
      ]
      Body unTypedBody: body typedBody2: {}!
    ]

   ^| tok kind
    | TokenType.OpenBrace => [
      body = manyPrimary do
      .match: TokenType.CloseBrace
      ListLiteral token: tok elements: body
    ]
    | TokenType.OpenBraceHash => [
      body = manyPrimary do
      .match: TokenType.CloseBrace

      body unTypedBody count % 2 != 0 ifTrue: [
        TO DO: "elements count in Map should be even"
      ]

      MapLiteral token: tok elements: body
    ]
    | TokenType.OpenParenHash => [
      body = manyPrimary do
      .match: TokenType.CloseParen
      SetLiteral token: tok elements: body
    ]
    |=> null
  ]


  /// collection {} #{} #()
  /// lambda
  /// or identifier | nullableIdentifier
  /// simple literal 1 "" '' true false 1.1 1.1f
  on primary(tok): Token -> Expr? = [

    .literalExprFrom: tok, unpack: [
      ^it
    ]

    tok isNullOrIdentifier ifTrue: [
      ^.parseDotSeparatedIdentifier: tok
    ]

    .parseCollection: tok, unpack: [
      ^it
    ]

    tok kind == TokenType.OpenBracket ifTrue: [
      ^ .parseCodeBlock: tok
    ]

    ^ null
  ]

  // `[` already eaten
  on parseCodeBlock(tok): Token -> CodeBlock = [
    args <- List(NameAndAstType) val: {} // [a::Int -> a inc]
    untypedArgs <- List(Identifier) val: {} // [a, b -> a + b]
    savePoint = current

    | .peek kind
    | TokenType.IdentifierDoubleColon | TokenType.IdentifierColon => [
      identsWithAst = .parseNameSingleOrDoubleColonAstTypePair: .step separatedBy: TokenType.Comma
      args <- identsWithAst
    ]
    | TokenType.Identifier => [
      idents = .parseManyIdentSeparated: TokenType.Comma
      untypedArgs <- idents
    ]
    |=> []

    wasArrow = .match: TokenType.ReturnArrow

    wasArrow not && untypedArgs isNotEmpty ifTrue: [
      // what if [a + b]
      // a is in untyped args, and body failing with + b
      // so restore
      .restoreFrom: savePoint
      untypedArgs <- {}
    ]

    // wait, just check if there was an arrow, if not, then restore from tok
    mut body = .parseManyExprUntilTok: TokenType.CloseBracket split: null

    result = CodeBlock
      args: args
      untypedArgs: untypedArgs
      body: body
      token: tok

    ^ result
  ]


  on parseManyExprUntilTok(until): TokenType split: TokenType? -> Body = [
    body::mut List::Expr = {}!

    mut c = true
    [c] whileTrue: [
      tok = .peek
      tok kind == until ifTrue: [
        .step // skip
        c <- false
      ] ifFalse: [
        // parse statement
        nextTok = .step
        statement = .nextExperssion: nextTok
        split unpack: [.match: it]
        body add: statement
        Unit
      ]
    ]

    result = Body unTypedBody: body typedBody2: {}!

    ^ result
  ]
  on parseManyExprSplitBy(split): TokenType -> List::Expr = [
    body::mut List::Expr = {}!

    mut c = true
    [c] whileTrue: [
      tok = .peek
      nextTok = .step
      statement = .nextExperssion: nextTok
      body add: statement

      c <- .match: split
    ]

    ^ body
  ]


  // return body untyped
  on parseBodyOrSingleExpr -> Body = [
    // tokens map: [it kind toString + "\t\t" + it lexeme], joinWith: "\n"
    ^ .match: TokenType.OpenBracket, ifTrue: [
      // parse body
      .parseManyExprUntilTok: TokenType.CloseBracket split: null
    ] ifFalse: [
      // single expression
      Body unTypedBody: {(.nextExperssion: .step)}! typedBody2: {}!
    ]
  ]

  /// a::Int (sep)? b::String
  on parseNameSingleOrDoubleColonAstTypePair(tok): Token separatedBy(sep): TokenType? -> List::NameAndAstType = [

    firstArg = NameAndAstType name: tok lexeme astType: .parseType
    sep unpack: [.match: it]

    kwArgs = {firstArg}!

    mut c = true
    [c] whileTrue: [
      tok = .peek
      singleOrDoubleColon = tok kind == TokenType.IdentifierDoubleColon || (tok kind == TokenType.IdentifierColon)
      singleOrDoubleColon ifTrue: [
        arg = NameAndAstType name: .step lexeme astType: .parseType
        sep unpack: [.match: it]

        kwArgs add: arg
        Unit
      ] ifFalse: [
        c <- false
      ]
    ]
    ^ kwArgs
  ]

  on restoreFrom(position): Int = [
    current <- position
  ]

  on skipComments = [
    [.peek kind == TokenType.Comment] whileTrue: [
      .step
    ]
  ]

  on parseMatch(tok): Token -> Match = [
    branches::mut List::MatchBranch = {}!
    mut elseBranch::Body? = null
    subject = .nextExperssion: .step

    parseBranches = [
      mut c = true
      exprConditions::mut List::Expr = {}!
      body::mut List::Expr = {}!

      // match ifs
      [c] whileTrue: [
        c <- .match: TokenType.If
        exprs = .parseManyExprSplitBy: TokenType.If
        exprs isEmpty ifTrue: [TO DO: "One or more conditions (separated by `|`) expected"]

        .matchAssert: TokenType.Then

        body = .parseBodyOrSingleExpr
        conditions = Body unTypedBody: (exprs drop: 1, toMutableList) typedBody2: {}!
        mb = MatchBranch condition: exprs first conditions: conditions body: body
        branches add: mb

        c <- .match: TokenType.If
      ]
      // match else
      .match: TokenType.Else, ifTrue: [
        elseBranch <- .parseBodyOrSingleExpr
      ]

    ] do

    ^ Match subject: subject branches: branches elseBranch: elseBranch token: tok
  ]
  on nextStatement -> Statement = [
    // .skipComments
    savePosition = current

    // there always EndOfFile, so this never happen
    tokens isEmpty ifTrue: [TO DO: "no tokens, file is empty"]
    tok = .step

    // optimization for expr that starts from 1 | ""
    .isLiteralExpr: tok, ifTrue: [
      ^ .nextExperssion: tok
    ]

    | tok kind
    | TokenType.Type => [^ .typeDeclaration: tok]
    | TokenType.Union => [^ .unionTypeDeclaration: tok]
    | TokenType.Constructor => [
      ^ .messageDeclarationOrExpression: .step savedPosition: savePosition isConstructor: true
    ]
    | TokenType.Identifier => [
      firstChar = tok lexeme first
      isTypeName = firstChar isUpperCase
      isTypeName ifTrue: [
        ^ .messageDeclarationOrExpression: tok savedPosition: savePosition isConstructor: false
      ] ifFalse: [
        ^ .nextExperssion: tok
      ]
    ]
    | TokenType.Mut => [
      nextTok = .peek
      isTypeName = | nextTok kind
                  | TokenType.Identifier
                  | TokenType.NullableIdentifier
                  | TokenType.IdentifierDoubleColon => [
                    firstChar = nextTok lexeme first
                    firstChar isUpperCase
                  ]
                  |=> false
      isTypeName ifTrue: [
        ^ .messageDeclarationOrExpression: tok savedPosition: savePosition isConstructor: false
      ] ifFalse: [
        ^ .nextExperssion: tok
      ]
    ]
    | TokenType.If => [
      ^.parseMatch: tok
    ]
    | TokenType.Return => [
      ^.parseReturnExpr: tok
    ]
    // commented since expression after is possible
    // | TokenType .OpenBracket => [
    //   ^.parseCodeBlock: tok
    // ]
    | TokenType.EndOfFile => [
      TO DO: "its impossible to be here, whileTrue from top stack goes until `done`"
    ]
    | TokenType.Invalid => [
      prevPrev = .peekAt: -2
      prev = .peekAt: -1
      cur = .peek
      TO DO: "Got invalid token after $prevPrev $prev $cur"
    ]
    | TokenType.IdentifierDoubleColon => [
      ^.parseVarDeclExpr: tok name: tok lexeme isMut: false // `::` for IdentifierDoubleColon is removed in lexer
    ]

    |=> [
      //"Warning: Parser: fallback to expr parsing with tok: $tok" echo
      ^ .nextExperssion: tok
    ]



    .peek kind != TokenType.EndOfFile, ifTrue: [
      .step // just to evade infinity loop
    ]



    ^ TO DO: "top level $tok is not allowed"
  ]

  /// `^ expr` | `expr`
  on nextExperssion(tok): Token -> Expr = [
    // return expr
   ^| tok kind
    | TokenType.Return => [
      e = .parseReturnExpr: tok
      e
    ]
    | TokenType.Mut => [
        name = .step // tok is mut here, so we need name
        ^ .parseVarDeclExpr: name name: name lexeme isMut: true
    ]
    |=> [

      // VarDeclaration: `x::Int`
      tok kind == TokenType.IdentifierDoubleColon ifTrue: [
        ^.parseVarDeclExpr: tok name: tok lexeme isMut: false
      ]

      // assign
      primaryReceiver = .primary: tok, unpackOrMsg: "nextExperssion: no primary ($tok)"
      > primaryReceiver
      .peek kind == TokenType.Assign, ifTrue: [
        ^.parseVarDeclExpr: tok name: primaryReceiver token lexeme isMut: false
      ]
      //TO DO: "ТАк посмотреть по истории как тут было последний раз в этом месте, весто праймари пробовать еще и запятую а потом уже выбрасывать и добавить тест с замятыми всех мастей"
      .peek kind == TokenType.AssignArrow, ifTrue: [
        .step
        value = .nextExperssion: .step
        | primaryReceiver
        | Identifier => [
          ^ Assign name: primaryReceiver name value: value token: tok
        ]
        |=> [
          TO DO: "<- expected identifier on the left side"
        ]
      ]
      // any expr
      firstExpr = .nextExperssion2: tok receiver: primaryReceiver

      // expr, expr
     ^| .peek kind
      | TokenType.Comma => [
        .step // skip comma
        .nextExperssion2: tok receiver: (ExprInBrackets value: firstExpr token: tok)
      ]
      |=> firstExpr
    ]
  ]

  on nextExperssion2(tok): Token receiver: Expr? -> Expr = [
    // return expr
    tok kind == TokenType.Return ifTrue: [
      ^ .parseReturnExpr: tok
    ]
    // message expr
    primaryUnaryBinary = .parsePrimaryOrUnaryOrBinary: tok receiver: receiver

   ^| .peek kind
    | TokenType.IdentifierColon => [

      result::mut List::NameAndExpr = {}!
      mut c = true
      // parse keyword
      [c] whileTrue: [
        colon = .peek
        | colon kind
        | TokenType.IdentifierColon => [

          result add:
            (NameAndExpr
              name: (Identifier fromToken: .step)
              expr: (.parsePrimaryOrUnaryOrBinary: .step))
          Unit

        ]
        |=> [
          c <- false
        ]
      ]
      // current debug

      KeywordMsg
        args: result toList
        receiver: primaryUnaryBinary
        token: tok
        kind: MessageSendKind.Call

    ]
    |=> primaryUnaryBinary
  ]


  // returns
  // Primary
  // UnaryMsg
  // BinaryMsg
  on parsePrimaryOrUnaryOrBinary(tok): Token -> Expr = .parsePrimaryOrUnaryOrBinary: tok receiver: null
  on parsePrimaryOrUnaryOrBinary(tok): Token receiver::Expr? -> Expr = [
    maybePrimary = | receiver
    | null => .primary: tok, unpackOrMsg: "nextExperssion: no primary ($tok)"
    |=> receiver

    next = .peek
    // single primary before new line
    // or in other words just `42`
    next line > tok line, ifTrue: [
      ^ maybePrimary
    ]

    // 1 ^

    | next kind
    // 1 ^inc
    | TokenType.Identifier => [
      // primary identifier

      // while there are identifiers - parse and save them, its a begining of unary
      tok = .peek
      | tok kind
      | TokenType.Identifier => [
        unaryList = .parseManyIdentSeparated: null
        unaryIdentifiers = unaryList,
                            map: [
                              NameAndExpr name: it expr: it
                            ]

        mut tempUnary = UnaryMsg
          selector: unaryList first
          kind: MessageSendKind.Call
          receiver: maybePrimary
          token: tok

        unaryList drop: 1, forEach: [
          newUnary = UnaryMsg
            selector: it
            kind: MessageSendKind.Call
            receiver: tempUnary
            token: tok
          tempUnary <- newUnary
        ]

        unary = tempUnary

        // 1 inc inc^
        .peek kind == TokenType.BinarySymbol, ifTrue: [
          // 1 inc inc^ + 2
          binaryList = .parseManyBinary
          binary = BinaryMsg
            args: binaryList toList
            receiver: unary
            token: maybePrimary token
          .peek kind == TokenType.IdentifierColon, ifTrue: [
            // 1 inc from: 2
            TO DO: "parse keyword after binary2"
          ]
          ^binary
        ]

        // .peek kind == TokenType.IdentifierColon, ifTrue: [
        //   // 1 inc from: 2
        //   TO DO: "parse keyword after unary"
        // ]
        ^ unary
        Unit
      ]
      | TokenType.BinarySymbol => [
        binaryList = .parseManyBinary

        binary = BinaryMsg
          args: binaryList toList
          receiver: maybePrimary
          token: maybePrimary token
        // statements add: binary
        // TODO нужно трекать уровень вложенности, и если вложенность больше 0, то не добавлять
        // .peek kind == TokenType.IdentifierColon, ifTrue: [
        //   // 1 inc from: 2
        //   TO DO: "parse keyword after binary1"
        // ]
        ^ binary
        Unit
      ]
      |=> [
        Unit
      ]

      TO DO: "parse unary"
    ]
    | TokenType.BinarySymbol => [
      // primary binary
      // 1 + 1 + 1
      // 1 + 1 sas: 2


      // 1 + ^2
      binaryList = .parseManyBinary

      binary = BinaryMsg
        args: binaryList
        receiver: maybePrimary
        token: maybePrimary token
      // statements add: binary

      // .peek kind == TokenType.IdentifierColon, ifTrue: [
      //   TO DO: "binary is a receiver for keyword"
      // ]

      ^ binary
      // kw possible

      // if while parsing binary got keyword than continue keyword parsing
    ]
    // | TokenType.IdentifierColon => [
    //   // primary keyword
    //   // 1 from: 2
    //   // 1 inc from: 2
    //   // 1 + 1 from: 2
    //   // binaryList = .parseManyBinary
    //   // binaryList debug
    //   TO DO: "parse simple keyword1"
    // ]
    |=> []

    ^ maybePrimary
  ]



  // / x| probably not just identifier without anything
  // / x = ...
  // / x inc ...
  // / X inc = []
  // / x + ...
  // / X + y::Int = []
  // / x that: ...
  // / X from::Int = []
  // / X from y:Int = []



  on match::TokenType -> Bool = [
    .peek kind == match, ifTrue: [
      .step
      ^ true
    ]

    ^ false
  ]

  /// match Identifier|Nullable|Error
  on matchAssertAnyIdent(error): String -> Token = [
    tok = .peek

    ^ tok isIdentifier ifTrue: [
      .step
      tok
    ] ifFalse: [
      TO DO: error
    ]
  ]

  on matchAssert::TokenType -> Token = [
    peek = .peek
    peek kind == matchAssert, ifTrue: [
      ^ .step
    ]
    kind = peek kind
    {line start} = peek
    ^ TO DO: "$matchAssert expected but got $peek at: $line:$start"
  ]

  on matchString::String -> Bool = [
    .peek lexeme == matchString, ifTrue: [
      .step
      ^ true
    ]

    ^ false
  ]

  on check(kind): TokenType = .peek kind == kind

  on done = .check: TokenType.EndOfFile

  on step -> Token = [
    x = .peek
    x kind == TokenType.EndOfFile, ifTrue: [
      TO DO: "step: EOF"
    ]
    current <- current inc
    ^ x
  ]
  on stepBack -> Token = [
    x = .peekAt: -1
    current <- current dec
    ^ x
  ]

  /// if current is out of bounds returns EOF
  /// otherwise returns token at current position
  on peek -> Token = [
    // current >= tokens count, ifTrue: [
    //   ^ Token eof
    // ]

    ^tokens at: current
  ]
  on peekAt(distance): Int -> Token = [
    realDistance = current + distance
    ^tokens at: realDistance
  ]
]
